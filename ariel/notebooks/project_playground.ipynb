{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import nltk\n",
    "from scipy import sparse\n",
    "import time\n",
    "import pdb\n",
    "\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/ariel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stop-words, stemmer\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data directory\n",
    "data_dir = '../../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###First extract training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(data_dir+\"testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size fo the testing set: 32648\n"
     ]
    }
   ],
   "source": [
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "print \"Size fo the testing set:\", len(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size fo the training set: 615512\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir+\"training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "\n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "labels = [element[-1] for element in training_set]\n",
    "training_set = [element[:-1] for element in training_set]\n",
    "print \"Size fo the training set:\", len(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Extract node information\n",
    "1. unique ID, \n",
    "2. publication year (between 1993 and 2003), \n",
    "3. title, \n",
    "4. authors, \n",
    "5. name of journal (not available for all papers), and \n",
    "6. abstract. Abstracts are already in lowercase, common English stopwords have been removed, and punctuation marks have been removed except for intra-word dashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(data_dir+\"node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "# [info_dict[el[0] = el[1:] for el in node_info]\n",
    "# IDs = [element[0] for element in node_info]\n",
    "info_dict = dict([[el[0], el[1:]] for el in node_info])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Training and testing with baseline classifier (linear SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_features_baseline(data_set, info_dict):\n",
    "    \n",
    "    ## Features for training\n",
    "    # number of overlapping words in title\n",
    "    overlap_title = []\n",
    "    # temporal distance between the papers\n",
    "    temp_diff = []\n",
    "    # number of common authors\n",
    "    comm_auth = []\n",
    "    \n",
    "    counter = 0\n",
    "    for (source, target) in data_set:\n",
    "\n",
    "        source_info = info_dict[source]\n",
    "        target_info = info_dict[target]\n",
    "\n",
    "        # convert to lowercase and tokenize\n",
    "        source_title = source_info[1].lower().split(\" \")\n",
    "        # remove stopwords\n",
    "        source_title = [token for token in source_title if token not in stpwds]\n",
    "        source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "        target_title = target_info[1].lower().split(\" \")\n",
    "        target_title = [token for token in target_title if token not in stpwds]\n",
    "        target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "        source_auth = source_info[2].split(\",\")\n",
    "        target_auth = target_info[2].split(\",\")\n",
    "\n",
    "        overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "        temp_diff.append(int(source_info[0]) - int(target_info[0]))\n",
    "        comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 10000 == 0:\n",
    "            print counter, \"training examples processsed\"\n",
    "    \n",
    "    features = np.array([overlap_title, temp_diff, comm_auth]).T.astype(float)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Divide training into train and test\n",
    "n = len(training_set)\n",
    "ntrain = 2*n/3\n",
    "ntest = n - ntrain\n",
    "train, test, label_train, label_test = train_test_split(training_set, labels, train_size=ntrain, test_size=ntest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 training examples processsed\n",
      "20000 training examples processsed\n",
      "30000 training examples processsed\n",
      "40000 training examples processsed\n",
      "50000 training examples processsed\n",
      "60000 training examples processsed\n",
      "70000 training examples processsed\n",
      "80000 training examples processsed\n",
      "90000 training examples processsed\n",
      "100000 training examples processsed\n",
      "110000 training examples processsed\n",
      "120000 training examples processsed\n",
      "130000 training examples processsed\n",
      "140000 training examples processsed\n",
      "150000 training examples processsed\n",
      "160000 training examples processsed\n",
      "170000 training examples processsed\n",
      "180000 training examples processsed\n",
      "190000 training examples processsed\n",
      "200000 training examples processsed\n",
      "210000 training examples processsed\n",
      "220000 training examples processsed\n",
      "230000 training examples processsed\n",
      "240000 training examples processsed\n",
      "250000 training examples processsed\n",
      "260000 training examples processsed\n",
      "270000 training examples processsed\n",
      "280000 training examples processsed\n",
      "290000 training examples processsed\n",
      "300000 training examples processsed\n",
      "310000 training examples processsed\n",
      "320000 training examples processsed\n",
      "330000 training examples processsed\n",
      "340000 training examples processsed\n",
      "350000 training examples processsed\n",
      "360000 training examples processsed\n",
      "370000 training examples processsed\n",
      "380000 training examples processsed\n",
      "390000 training examples processsed\n",
      "400000 training examples processsed\n",
      "410000 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# training features\n",
    "training_features = compute_features_baseline(train, info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize basic SVM\n",
    "classifier = svm.LinearSVC()\n",
    "# convert list of lists into array\n",
    "# documents as rows, unique words as columns (i.e., example as rows, features as columns)\n",
    "#training_features = np.array([overlap_title, temp_diff, comm_auth]).T.astype(float)\n",
    "# scale\n",
    "training_features = preprocessing.scale(training_features)\n",
    "# convert labels into integers then into column array\n",
    "labels_array = np.array(label_train)\n",
    "# initialize basic SVM\n",
    "classifier = svm.LinearSVC()\n",
    "# train\n",
    "classifier.fit(training_features, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 training examples processsed\n",
      "20000 training examples processsed\n",
      "30000 training examples processsed\n",
      "40000 training examples processsed\n",
      "50000 training examples processsed\n",
      "60000 training examples processsed\n",
      "70000 training examples processsed\n",
      "80000 training examples processsed\n",
      "90000 training examples processsed\n",
      "100000 training examples processsed\n",
      "110000 training examples processsed\n",
      "120000 training examples processsed\n",
      "130000 training examples processsed\n",
      "140000 training examples processsed\n",
      "150000 training examples processsed\n",
      "160000 training examples processsed\n",
      "170000 training examples processsed\n",
      "180000 training examples processsed\n",
      "190000 training examples processsed\n",
      "200000 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# training features\n",
    "testing_features = compute_features_baseline(test, info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_features = preprocessing.scale(testing_features)\n",
    "predictions_SVM = list(classifier.predict(testing_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.663012803954\n"
     ]
    }
   ],
   "source": [
    "acc = np.sum((np.array(predictions_SVM)==np.array(label_test)).astype(float))/len(test_labels)\n",
    "print \"Baseline accuracy:\", acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding features on abstract (words in common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide training into train and test\n",
    "n = len(training_set)\n",
    "ntrain = 2*n/3\n",
    "ntest = n - ntrain\n",
    "train, test, label_train, label_test = train_test_split(training_set, labels, train_size=ntrain, test_size=ntest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_word_doc_mat(data_set, info_dict):\n",
    "    \n",
    "    \n",
    "    unique_doc = np.unique(np.array(data_set).ravel())\n",
    "    word_dict = {}\n",
    "    doc_dict = {}\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    prev_len_col = 0\n",
    "    \n",
    "    counter = 0\n",
    "    for i, doc in enumerate(unique_doc):\n",
    "\n",
    "        doc_dict[doc] = i \n",
    "        abst = info_dict[doc][-1]\n",
    "        \n",
    "        # convert to lowercase and tokenize\n",
    "        abst = abst.lower().split(\" \")\n",
    "        # remove stopwords\n",
    "        abst = [token for token in abst if token not in stpwds]\n",
    "        abst = [stemmer.stem(token) for token in abst]\n",
    "        tmplist = []\n",
    "        for w in abst:\n",
    "            if w not in word_dict:\n",
    "                word_dict[w] = len(word_dict)\n",
    "            idx = word_dict[w]\n",
    "            tmplist.append(idx)\n",
    "        data += [1./sum(tmplist)]*len(abst)\n",
    "        col += tmplist\n",
    "        row += [i]*len(abst)\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 5000 == 0:\n",
    "            print counter, \"training doc processsed\"\n",
    "\n",
    "    mat = sparse.csr_matrix( (data, (row, col)) , shape=(len(unique_doc), len(word_dict)))\n",
    "    \n",
    "    return mat, word_dict, doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 training doc processsed\n",
      "10000 training doc processsed\n",
      "15000 training doc processsed\n",
      "20000 training doc processsed\n",
      "25000 training doc processsed\n",
      "26.2572929859\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "mat, word_dict, doc_dict = train_word_doc_mat(train, info_dict)\n",
    "print time.time() - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_features_abstract(data_set, doc_word_mat, info_dict, doc_dict):\n",
    "    \n",
    "    ## Features for training\n",
    "    # number of overlapping words in title\n",
    "    overlap_title = []\n",
    "    # temporal distance between the papers\n",
    "    temp_diff = []\n",
    "    # number of common authors\n",
    "    comm_auth = []\n",
    "    # abstract similarities\n",
    "    abstract_feat = []\n",
    "        \n",
    "    counter = 0\n",
    "    for (source, target) in data_set:\n",
    "\n",
    "        source_info = info_dict[source]\n",
    "        target_info = info_dict[target]\n",
    "\n",
    "        # convert to lowercase and tokenize\n",
    "        source_title = source_info[1].lower().split(\" \")\n",
    "        # remove stopwords\n",
    "        source_title = [token for token in source_title if token not in stpwds]\n",
    "        source_title = [stemmer.stem(token) for token in source_title]\n",
    "\n",
    "        target_title = target_info[1].lower().split(\" \")\n",
    "        target_title = [token for token in target_title if token not in stpwds]\n",
    "        target_title = [stemmer.stem(token) for token in target_title]\n",
    "\n",
    "        source_auth = source_info[2].split(\",\")\n",
    "        target_auth = target_info[2].split(\",\")\n",
    "\n",
    "        overlap_title.append(len(set(source_title).intersection(set(target_title))))\n",
    "        temp_diff.append(int(source_info[0]) - int(target_info[0]))\n",
    "        comm_auth.append(len(set(source_auth).intersection(set(target_auth))))\n",
    "        \n",
    "        # abstract\n",
    "        ids = doc_dict[source]\n",
    "        idt = doc_dict[target]\n",
    "        dist = float(doc_word_mat[ids,:].toarray().dot(doc_word_mat[idt,:].toarray().T))\n",
    "        abstract_feat.append(dist)\n",
    "        \n",
    "        counter += 1\n",
    "        if counter % 10000 == 0:\n",
    "            print counter, \"training examples processsed\"\n",
    "    \n",
    "    features = np.array([overlap_title, temp_diff, comm_auth, abstract_feat]).T.astype(float)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 training examples processsed\n",
      "20000 training examples processsed\n",
      "30000 training examples processsed\n",
      "40000 training examples processsed\n",
      "50000 training examples processsed\n",
      "60000 training examples processsed\n",
      "70000 training examples processsed\n",
      "80000 training examples processsed\n",
      "90000 training examples processsed\n",
      "100000 training examples processsed\n",
      "110000 training examples processsed\n",
      "120000 training examples processsed\n",
      "130000 training examples processsed\n",
      "140000 training examples processsed\n",
      "150000 training examples processsed\n",
      "160000 training examples processsed\n",
      "170000 training examples processsed\n",
      "180000 training examples processsed\n",
      "190000 training examples processsed\n",
      "200000 training examples processsed\n",
      "210000 training examples processsed\n",
      "220000 training examples processsed\n",
      "230000 training examples processsed\n",
      "240000 training examples processsed\n",
      "250000 training examples processsed\n",
      "260000 training examples processsed\n",
      "270000 training examples processsed\n",
      "280000 training examples processsed\n",
      "290000 training examples processsed\n",
      "300000 training examples processsed\n",
      "310000 training examples processsed\n",
      "320000 training examples processsed\n",
      "330000 training examples processsed\n",
      "340000 training examples processsed\n",
      "350000 training examples processsed\n",
      "360000 training examples processsed\n",
      "370000 training examples processsed\n",
      "380000 training examples processsed\n",
      "390000 training examples processsed\n",
      "400000 training examples processsed\n",
      "410000 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "# training features\n",
    "training_features = build_features_abstract(train, mat, info_dict, doc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scale\n",
    "training_features = preprocessing.scale(training_features)\n",
    "# convert labels into integers then into column array\n",
    "labels_array = np.array(label_train)\n",
    "# initialize basic SVM\n",
    "classifier = svm.LinearSVC()\n",
    "# train\n",
    "classifier.fit(training_features, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_word_doc_mat(data_set, info_dict, word_dict):\n",
    "    \n",
    "    unique_doc = np.unique(np.array(data_set).ravel())\n",
    "    mat = sparse.csr_matrix((len(unique_doc), len(word_dict)), dtype=float)\n",
    "    doc_dict = {}\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    prev_len_col = 0\n",
    "    \n",
    "    counter = 0\n",
    "    for i, doc in enumerate(unique_doc):\n",
    "\n",
    "        doc_dict[doc] = i \n",
    "        abst = info_dict[doc][-1]\n",
    "        \n",
    "        # convert to lowercase and tokenize\n",
    "        abst = abst.lower().split(\" \")\n",
    "        # remove stopwords\n",
    "        abst = [token for token in abst if token not in stpwds]\n",
    "        abst = [stemmer.stem(token) for token in abst]\n",
    "        tmplist = []\n",
    "        for w in abst:\n",
    "            if w in word_dict:\n",
    "                idx = word_dict[w]\n",
    "                tmplist.append(idx)\n",
    "        data += [1./sum(tmplist)]*len(tmplist)\n",
    "        col += tmplist\n",
    "        row += [i]*len(tmplist)\n",
    "\n",
    "        counter += 1\n",
    "        if counter % 100 == 0:\n",
    "            #print counter, \"training examples processsed\"\n",
    "            mat += sparse.csr_matrix( (data, (row, col)) , shape=(len(unique_doc), len(word_dict)))\n",
    "            row = []\n",
    "            col = []\n",
    "            data = []\n",
    "    \n",
    "    return mat, doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing features\n",
    "mat_test, doc_dict_test = test_word_doc_mat(test, info_dict, word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 training examples processsed\n",
      "20000 training examples processsed\n",
      "30000 training examples processsed\n",
      "40000 training examples processsed\n",
      "50000 training examples processsed\n",
      "60000 training examples processsed\n",
      "70000 training examples processsed\n",
      "80000 training examples processsed\n",
      "90000 training examples processsed\n",
      "100000 training examples processsed\n",
      "110000 training examples processsed\n",
      "120000 training examples processsed\n",
      "130000 training examples processsed\n",
      "140000 training examples processsed\n",
      "150000 training examples processsed\n",
      "160000 training examples processsed\n",
      "170000 training examples processsed\n",
      "180000 training examples processsed\n",
      "190000 training examples processsed\n",
      "200000 training examples processsed\n"
     ]
    }
   ],
   "source": [
    "testing_features = build_features_abstract(test, mat_test, info_dict, doc_dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_features = preprocessing.scale(testing_features)\n",
    "predictions_SVM = list(classifier.predict(testing_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.726730385873\n"
     ]
    }
   ],
   "source": [
    "acc = np.sum((np.array(predictions_SVM)==np.array(label_test)).astype(float))/len(label_test)\n",
    "print \"Baseline accuracy:\", acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.726749881806\n",
      "3 0.727368877668\n",
      "5 0.728012243446\n",
      "7 0.728314430402\n",
      "9 0.728553255577\n"
     ]
    }
   ],
   "source": [
    "# Vary parameters\n",
    "Ctry = np.arange(1,10,2)\n",
    "for C in Ctry:\n",
    "    # initialize basic SVM\n",
    "    classifier = svm.LinearSVC(C=C)\n",
    "    # train\n",
    "    classifier.fit(training_features, labels_array)\n",
    "    predictions_SVM = list(classifier.predict(testing_features))\n",
    "    acc = np.sum((np.array(predictions_SVM)==np.array(label_test)).astype(float))/len(label_test)\n",
    "    print C, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# retrieve abstracts and process\n",
    "unique_doc = np.unique(np.array(training_set).ravel())\n",
    "abst_corpus = []\n",
    "doc_dict = {}\n",
    "for i, doc in enumerate(unique_doc):\n",
    "    doc_dict[doc] = i\n",
    "    abst = info_dict[doc][-1]\n",
    "    # convert to lowercase and tokenize\n",
    "    abst = abst.lower().split(\" \")\n",
    "    # remove stopwords\n",
    "    abst = [token for token in abst if token not in stpwds]\n",
    "    abst = [stemmer.stem(token) for token in abst]\n",
    "    abst_corpus.append(abst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vocabulary \n",
    "corpus = [str(\" \".join(abstract)) for abstract in abst_corpus]\n",
    "u_words = np.unique(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9612192 9512053 1\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "s, t = train[i]\n",
    "l = label_train[i]\n",
    "print s, t, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abst_s = info_dict[s][-1]\n",
    "abst_t = info_dict[t][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in this lecture we review some of the recent developments in string theory on an introductory and qualitative level in particular we focus on s-t-u dualities of toroidally compactified ten-dimensional string theories and outline the connection to m-theory dualities among string vacua with less supersymmetries in six and four space-time dimensions is discussed and the concept of f-theory is briefly presented lecture given by j louis at the workshop on gauge theories applied supersymmetry and quantum gravity imperial college london uk july 5 10 1996\n",
      "the conditions for the cancellation of all gauge gravitational and mixed anomalies of n 1 supersymmetric models in six dimensions are reviewed and illustrated by a number of examples of particular interest are models that cannot be realized perturbatively in string theory an example of this type which we verify satisfies the anomaly cancellation conditions is the k3 compactification of the so 32 theory with small instantons recently proposed by witten when the instantons coincide it has gauge group so 32 times sp 24 two new classes of models for which non-perturbative string constructions are not yet known are also presented they have gauge groups so 2n 8 times sp n and su n times su n where n is an arbitrary positive integer\n"
     ]
    }
   ],
   "source": [
    "print abst_s\n",
    "print abst_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abst_pair = []\n",
    "for abst in [abst_s, abst_t]:    \n",
    "    abst = abst.lower().split(\" \")\n",
    "    # remove stopwords\n",
    "    abst = [token for token in abst if token not in stpwds]\n",
    "    abst = [stemmer.stem(token) for token in abst]\n",
    "    abst_pair.append(abst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = doc_dict[s]\n",
    "idt = doc_dict[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inner_prod = (mat[ids,:].toarray()*mat[idt,:].toarray()).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "common_w = np.argsort(inner_prod)[::-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.48327788e-09,   1.48327788e-09,   7.41638938e-10,\n",
       "         2.47212979e-10,   2.47212979e-10])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inner_prod[common_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20886"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict.values()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'theori', u'string', u'gaug', u'review', u'recent']\n"
     ]
    }
   ],
   "source": [
    "print [word_dict.keys()[np.where(word_dict.values()==idx)[0].squeeze()] for idx in common_w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step: keywords extraction\n",
    "\n",
    "Keywords are chosen among words shared between linked abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_keywords(train, labels, mat, doc_dict, n=3):\n",
    "    \n",
    "    for ((s, t), l) in zip(train, labels):\n",
    "        \n",
    "        if l==1:\n",
    "            ids = doc_dict[s]\n",
    "            idt = doc_dict[t]\n",
    "            inner_prod = (mat[ids,:].toarray()*mat[idt,:].toarray()).squeeze()\n",
    "            common_w = np.argsort(inner_prod)[::-1][:n]\n",
    "            prod_val = inner_prod[common_w]\n",
    "            common_w = common_w[prod_val>0]\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
